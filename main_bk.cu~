#include "util.h"

#include <stdio.h>
#include <assert.h>
#include <iostream>
//#include <glog/logging.h>
#include <cuda_runtime.h>
#include "cusparse.h"
#include <thrust/device_ptr.h>
#include <thrust/copy.h>
#include <thrust/execution_policy.h>
#include <time.h>
#include <chrono>
#include <thrust/remove.h>
#include "cuCompactor.cuh"

const char* db_input = "./data/d_Db_input.txt.bin";
const char* pad_input = "./data/d_ht_pad_input.txt.bin";
const char* dist_input = "./data/d_outputdist_input.txt.bin";
const char* dist_output = "./data/d_outputdist_output.txt.bin";

const int batch = 12;
const int voc = 50000;
const int embed = 1001;

const int block_size = 32;

#define LOG(INFO) std::cout
#define CHECK assert
#define IDX2C(i,j,ld) (((j)*(ld))+(i))


struct non_negative
{
  __host__ __device__
  bool operator()(const int x)
  {
    return x >= 0;
  }
};

/**
 * Matrix multiplication (CUDA Kernel) on the device: C = A^T * B
 * A = k x m
 * B = k x n
 * All matrice are column major
 */
template <int BLOCK_SIZE>
__global__ void
matrixMulCUDA(float *C, float *C_input, float *A, float *B, 
	      const int M, const int N, const int K) {
  int cx = threadIdx.x + blockIdx.x*blockDim.x;
  int cy = threadIdx.y + blockIdx.y*blockDim.y;
  // Csub is used to store the element of the block sub-matrix
  // that is computed by the thread
  float Csub = 0.f;
  float Cinput = 0.f;
  float threshold = 0;
  if (cx < M && cy < N) {
    Cinput = C_input[cy*M+cx];
  }
  
  // Loop over all the sub-matrices of A and B
  // required to compute the block sub-matrix
  __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
  __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

  for (int ak = threadIdx.y, bk = threadIdx.x; ;
       ak += BLOCK_SIZE, bk += BLOCK_SIZE) {
    
    /*// Load the matrices from device memory*/
    /*// to shared memory; each thread loads*/
    /*// one element of each matrix*/
    if (ak < K && cx < M) {
      As[threadIdx.y][threadIdx.x] = A[cx*K+ak]; //As = A
    } else {
      As[threadIdx.y][threadIdx.x] = 0; 
    }
    if (bk < K && cy < N){
      Bs[threadIdx.y][threadIdx.x] = B[cy*K+bk]; //Bs = B^T
    } else {
      Bs[threadIdx.y][threadIdx.x] = 0; 
    }
    
    // Synchronize to make sure the matrices are loaded
    __syncthreads();
    
    // Multiply the two matrices together;
    // each thread computes one element
    // of the block sub-matrix
    
    /*
      if (cx < M && cy < N && Cinput > threshold) {
      #pragma unroll
      for (int k = 0; k < BLOCK_SIZE; ++k) {
      Csub += As[k][threadIdx.x] * Bs[threadIdx.y][k];
      } 
      }
    */
    
    /*// Synchronize to make sure that the preceding*/
    /*// computation is done before loading two new*/
    /*// sub-matrices of A and B in the next iteration*/
    //__syncthreads();
    
    if (ak / BLOCK_SIZE == K/BLOCK_SIZE){
      break;
    }
    
  }
  
  // Write the block sub-matrix to device memory;
  // each thread writes one element

  if (cx < M && cy < N) {
    if (Cinput>threshold){
      C[cy*M+cx] = Csub;
    } else {
      C[cy*M+cx] = -1000;
    }
    
  }
}


// C = A - B if A[i] == -1000, C[i] == 0;
// <<<1, 1024>>>
__global__ 
void special_matrix_sum(float *C, float *A, float *B, int count) {
#pragma unroll
  for (int i = threadIdx.x; i< count; i += blockDim.x) {
    if (A[i] == -1000){
      C[i] = 0;
    } else {
      C[i] = A[i] - B[i];
    }
  }
}
  

// dense matrix 2 array
// matrix [vocab, batch]
// <<<vocab/256,256>>>
__global__
void dense2array(float *matrix, int vocab_size, int batch_size, float *array, int* index_array, int topn, int threshold){
  int vocab_index = threadIdx.x + blockIdx.x * blockDim.x;
  if (vocab_index < vocab_size){
    if (vocab_index < topn){
      array[vocab_index] = 1.f;
      index_array[vocab_index] = vocab_index;
    } else {
      float dest_val = 0;
      for (int batch_index = 0 ; batch_index < batch_size; batch_index ++){
	float val = matrix[batch_index * vocab_size + vocab_index];
	if (val >= threshold){
	  dest_val = 1;
	  break;
	}
      }
      array[vocab_index] = dest_val;
      if (dest_val == 0){
	index_array[vocab_index] = -1;
      } else {
	index_array[vocab_index] = vocab_index;
      }
  
    }
  }
}

struct Lock {
  int *mutex;
  Lock(){
    cudaMalloc((void **)&mutex, sizeof(int));
    cudaMemset(mutex, 0, sizeof(int));
  }

  ~Lock(){
    cudaFree(mutex);
  }
  
  __device__ void lock(){
    while(atomicCAS(mutex,0,1)!=0);
  }

  __device__ void unlock(){
    atomicExch(mutex, 0);
  }

};



// dense matrix 2 array
// matrix [vocab, batch]
// <<<vocab/256,256>>>
__global__ void dense2array_compact(float *matrix, int vocab_size, int batch_size, float *array, int* index_array, int topn, int threshold, Lock lock, int *n){
  const int nthreads = 256;
  int vocab_index = threadIdx.x + blockIdx.x * blockDim.x;

  __shared__ int temp_n_global;
  __shared__ int index_array_shared[nthreads];
  __shared__ int index_array_shared_2[nthreads];
  __shared__ int temp_n_globals[nthreads/32];
  __shared__ int temp_ns[nthreads/32];
  if (vocab_index < vocab_size){
    if (vocab_index < topn){
      array[vocab_index] = 1.f;
      index_array_shared[threadIdx.x] = vocab_index;
      //index_array[vocab_index] = vocab_index;
    } else {
      float dest_val = 0;
      for (int batch_index = 0 ; batch_index < batch_size; batch_index ++){
	float val = matrix[batch_index * vocab_size + vocab_index];
	if (val >= threshold){
	  dest_val = 1;
	  break;
	}
      }
      array[vocab_index] = dest_val;
      if (dest_val == 0){
	index_array_shared[threadIdx.x] = -1;
      } else {
	index_array_shared[threadIdx.x] = vocab_index;
      }
    }
  } else {
    index_array_shared[threadIdx.x] = -1;
  }
  if (threadIdx.x % 32 == 0){
    int temp_n = 0;
    for (int i = threadIdx.x; i < threadIdx.x + 32; i++){
      if (index_array_shared[i] != -1){
	index_array_shared_2[threadIdx.x + temp_n] = index_array_shared[i];
	temp_n += 1;
      }
    }
    temp_n_globals[threadIdx.x/32] = atomicAdd(n, temp_n);
    temp_ns[threadIdx.x / 32] = temp_n;
  }
  
  if (threadIdx.x % 32 < temp_ns[threadIdx.x / 32]){
    index_array[threadIdx.x % 32 + temp_n_globals[threadIdx.x / 32]] = index_array_shared_2[threadIdx.x];
  }  

}




// rowIdx: [nnz]
// <<<((nnz+31)/32,(embed+32-1)/32)), (32,32)>>>
__global__
void fill_new_db_2(float *d_new_db, float* d_db, int *rowIdx, int embed, int nnz){
  __shared__ float tile[32][33];
  // read from [embed, vocab]
  int row_index = threadIdx.y + blockIdx.x * blockDim.x;
  if (row_index < nnz){
    int vocab_index = rowIdx[row_index];
    int x = threadIdx.x + blockIdx.y * blockDim.y;
    if (x<embed){
      tile[threadIdx.y][threadIdx.x] = d_db[IDX2C(x,vocab_index,embed)];
    }
  }
  __syncthreads();
  int x = threadIdx.x + blockIdx.x * blockDim.x; 
  int y = threadIdx.y + blockIdx.y * blockDim.y;
  if (x < nnz && y<embed){  
    d_new_db[IDX2C(x,y,nnz)] = tile[threadIdx.x][threadIdx.y];
  }
}

// rowIdx: [nnz]
// <<<((nnz+31)/32,(embed+32-1)/32)), (32,8)>>>
__global__
void fill_new_db_3(float *d_new_db, float* d_db, int *rowIdx, int embed, int nnz){
  __shared__ float tile[32][33];
  int stride = 8;
  // read from [embed, vocab]
  int x = threadIdx.x + blockIdx.y * 32;
  int row_index = threadIdx.y + blockIdx.x * 32;
  if (x < embed){
    for (int i = 0; i< 32 && row_index + i < nnz ; i+=stride){
    int vocab_index = rowIdx[row_index + i];
    tile[threadIdx.y + i][threadIdx.x] = d_db[IDX2C(x,vocab_index,embed)];
    }
  }
  
  __syncthreads();

  x = threadIdx.x + blockIdx.x * 32; 
  int y = threadIdx.y + blockIdx.y * 32;
  if (x < nnz){
    for (int i = 0; i < 32 && y+i < embed; i += stride ){
      d_new_db[IDX2C(x,y+i,nnz)] = tile[threadIdx.x][threadIdx.y + i];
    }
  }
}

__global__
void compact_gpu( int *d_input, int *d_output, int size){
  if (threadIdx.x == 0){
    int k = 0;
    for (int i =0 ; i<size; i++){
      if (d_input[i] >= 0) {
	d_output[k] = d_input[i];
	k+=1;
      }
    }

  }
}

void compact_thrust(int* h_input, int* h_output, int *d_input, int *d_output, int size){
  cudaMemcpy(h_input, d_input, size*sizeof(int), cudaMemcpyDeviceToHost);
    thrust::copy_if(h_input,h_input+size, h_output, non_negative());
    cudaMemcpy(d_output, h_output, size*sizeof(int), cudaMemcpyHostToDevice);
}


void compact(int* h_input, int* h_output, int *d_input, int *d_output, int size){
    cudaMemcpy(h_input, d_input, size*sizeof(int), cudaMemcpyDeviceToHost);
    int k = 0;
    for (int i =0 ; i<size; i++){
      if (h_input[i] >= 0) {
	h_output[k] = h_input[i];
	k+=1;
      }
    }
    cudaMemcpy(d_output, h_output, k*sizeof(int), cudaMemcpyHostToDevice);
}


// rowIdx: [nnz]
// <<<(nnz+2-1)/2, (512,2)>>>
__global__
void fill_new_db(float *d_new_db, float* d_db, int *rowIdx, int embed, int nnz){
  int row_index = threadIdx.y + blockIdx.x * blockDim.y;
  if (row_index < nnz){
    int vocab_index = rowIdx[row_index];
    for (int i = threadIdx.x ; i < embed ; i+=blockDim.x){
      //d_new_db[row_index + i*nnz] = d_db[vocab_index*embed + i]; //T
      d_new_db[row_index*embed + i] = d_db[vocab_index*embed + i];
      //d_new_db[row_index*embed + i] = d_db[row_index*embed + i];

      
      //s += d_db[vocab_index*embed + i];
    }
  }
}

// <<<(n,batch_size), 1024 >>>
template<typename dType>
__global__
void fill_number(dType *d_dist, int vocab_size, int batch_size, dType val){
  int batch_index = blockIdx.y;
  for (int vocab_index = threadIdx.x + blockIdx.x * blockDim.x; vocab_index < vocab_size; vocab_index += gridDim.x * blockDim.x){
    d_dist[IDX2C(vocab_index, batch_index, vocab_size)] = val;
  }
}


// d_dist_buf : [vocab, batch_size]
// d_dist_shrink : [nnz, batch_size]
// <<<((nnz+1024-1)/1024,batch_size), 1024 >>>
__global__
void array2dense(float *d_dist, float* d_dist_shrink, int* d_rowIdx, int vocab_size, int nnz){
  int batch_index = blockIdx.y;
  for (int row_index = threadIdx.x + blockIdx.x * blockDim.x; row_index < nnz; row_index += gridDim.x * blockDim.x){
    int vocab_index = d_rowIdx[row_index];
    d_dist[IDX2C(vocab_index, batch_index, vocab_size)] = d_dist_shrink[IDX2C(row_index,batch_index, nnz)];
  }
}
		 

template<typename dType>
void print_matrix_gpu(dType *d_matrix,int rows,int cols, int row_start, int row_end, int col_start, int col_end) {
  dType * h_matrix = (dType *)malloc(rows*cols*sizeof(dType));
  cudaMemcpy(h_matrix, d_matrix, rows*cols*sizeof(dType), cudaMemcpyDeviceToHost);
  for(int i=row_start; i<row_end; i++) {
    for(int j=col_start; j<col_end; j++) {
      std::cout << h_matrix[i + j*rows] << " ";
    }
    std::cout << "\n";
  }
  std::cout << "\n";
  free(h_matrix);
}


int main() {
  FILE *fp = NULL;
  void *db_buf, *pad_buf, *dist_buf, *d_new_db_buf;
  void *d_db_buf, *d_pad_buf, *d_dist_buf_input,*d_dist_buf_res,*d_dist_buf_output, *d_dist_buf_1, *d_dist_buf_2, *d_array_buf;
  int *d_index_buf, *h_input, *h_output;
  h_input = (int *)malloc(voc * sizeof(int));
  h_output = (int *)malloc(voc * sizeof(int));

  { //read Db
    CHECK(fp = fopen(db_input, "rb"));
    int db_input_meta[2];
    CHECK(fread(db_input_meta, sizeof(int), 2, fp) == 2);
    CHECK(db_input_meta[0] == embed); //<< db_input_meta[0];
    CHECK(db_input_meta[1] == voc);// << db_input_meta[1];
    int count = embed*voc;
    db_buf = malloc(count*sizeof(float));
    CHECK(fread(db_buf, sizeof(float), count, fp) == count);
    checkCudaError(cudaMalloc(&d_db_buf, count*sizeof(float)));
    checkCudaError(cudaMemcpy(d_db_buf, db_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));
  }

  { // read ht_pad
    CHECK(fp = fopen(pad_input, "rb"));
    int pad_input_meta[2];
    CHECK(fread(pad_input_meta, sizeof(int), 2, fp) == 2);
    CHECK(pad_input_meta[0] == embed) ;//<< pad_input_meta[0];
    CHECK(pad_input_meta[1] == batch);// << pad_input_meta[1];
    int count = embed*batch;
    pad_buf = malloc(count*sizeof(float));
    CHECK(fread(pad_buf, sizeof(float), count, fp) == count);
    checkCudaError(cudaMalloc(&d_pad_buf, count*sizeof(float)));
    checkCudaError(cudaMemcpy(d_pad_buf, pad_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));
  }

  { // read dist_input
    CHECK(fp = fopen(dist_input, "rb"));
    int dist_input_meta[2];
    CHECK(fread(dist_input_meta, sizeof(int), 2, fp) == 2);
    CHECK(dist_input_meta[0] == voc);// << dist_input_meta[0];
    CHECK(dist_input_meta[1] == batch);// << dist_input_meta[1];
    int count = voc*batch;
    dist_buf = malloc(count*sizeof(float));
    CHECK(fread(dist_buf, sizeof(float), count, fp) == count);
    checkCudaError(cudaMalloc(&d_dist_buf_input, count*sizeof(float)));
    checkCudaError(cudaMalloc(&d_dist_buf_1, count*sizeof(float)));
    checkCudaError(cudaMalloc(&d_dist_buf_2, count*sizeof(float)));
    checkCudaError(cudaMalloc(&d_dist_buf_res, count*sizeof(float)));
    checkCudaError(cudaMalloc(&d_array_buf, voc*sizeof(float)));
    checkCudaError(cudaMalloc(&d_index_buf, voc*sizeof(int)));
    
    checkCudaError(cudaMemcpy(d_dist_buf_input, dist_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));
    checkCudaError(cudaMemcpy(d_dist_buf_1, dist_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));
    checkCudaError(cudaMemcpy(d_dist_buf_2, dist_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));

  }

  { // read dist_output
    CHECK(fp = fopen(dist_output, "rb"));
    int dist_input_meta[2];
    CHECK(fread(dist_input_meta, sizeof(int), 2, fp) == 2);
    CHECK(dist_input_meta[0] == voc);// << dist_input_meta[0];
    CHECK(dist_input_meta[1] == batch);// << dist_input_meta[1];
    int count = voc*batch;
    dist_buf = malloc(count*sizeof(float));
    CHECK(fread(dist_buf, sizeof(float), count, fp) == count);
    checkCudaError(cudaMalloc(&d_dist_buf_output, count*sizeof(float)));
    checkCudaError(cudaMemcpy(d_dist_buf_output, dist_buf, count*sizeof(float),
                              cudaMemcpyHostToDevice));
  }

  { //dense2array
    cudaEvent_t start, stop;
    checkCudaError(cudaEventCreate(&start));
    checkCudaError(cudaEventCreate(&stop));
    int topn = 1000;
    int threshold = 5;
    int thread_size = 256;
    dim3 threads(thread_size);
    dim3 grid((voc+threads.x-1)/threads.x);
    float msecTotal = 0.0f;
    int Test = 1;
    

    int nnz, *cscRowIndA;
    int* d_nnz, *h_nnz ; 
    float fnnz;
    
    cudaMalloc((void **)&d_nnz, sizeof(int));
    cudaMemset(d_nnz, 0, sizeof(int));
    h_nnz = (int *)malloc(sizeof(int));

    cudaMalloc((void **)&cscRowIndA, voc*sizeof(int));
    cudaMalloc(&d_new_db_buf, voc*embed*sizeof(float));
   
    cublasHandle_t cublasHandle;
    checkCublasError(cublasCreate(&cublasHandle));
    float alpha = 1.f, beta = 0.f;


    thrust::device_ptr<int> thrust_index_buf((int*)d_index_buf);

    Lock lock;
    
    checkCudaError(cudaEventRecord(start, NULL));

    cudaDeviceSynchronize();
    std::chrono::time_point<std::chrono::system_clock> start2 = std::chrono::system_clock::now();
    
    int path = 1;
    
    for (int i = 0; i < Test; i++) {
      if (path == 0){
      dense2array<<<grid, threads>>>((float*)d_dist_buf_input, voc, batch, (float *)d_array_buf, (int *)d_index_buf, topn,threshold); //0.02ms
      cublasSasum(cublasHandle, voc, (float*)d_array_buf, 1, &fnnz); // 0.01ms
      nnz = std::floor(fnnz);
      cudaMemset(cscRowIndA, 0, voc * sizeof(int));
      compact(h_input,h_output,d_index_buf,cscRowIndA,voc);
      } else {
      cudaMemset(cscRowIndA, 0, voc * sizeof(int));
      dense2array_compact<<<grid, threads>>>((float*)d_dist_buf_input, voc, batch, (float *)d_array_buf, (int *)cscRowIndA, topn,threshold, lock, d_nnz); //0.02ms
      
      cudaMemcpy(h_nnz, d_nnz, sizeof(int), cudaMemcpyDeviceToHost);
      nnz = h_nnz[0];
      
      }
      
      int thread_x = 256; 
      int thread_y = 256 / thread_x;

      //cudaMemcpy(d_new_db_buf, d_db_buf, nnz * embed * sizeof(float), cudaMemcpyDeviceToDevice);
      fill_new_db<<<dim3((nnz + thread_y-1)/thread_y),dim3(thread_x,thread_y)>>>((float *)d_new_db_buf, (float *)d_db_buf,cscRowIndA,embed, nnz); // 0.84 ms
      
      cublasSgemm(cublasHandle,
		  CUBLAS_OP_T, CUBLAS_OP_N,
		  nnz, batch, embed, &alpha,
		  (float*)d_new_db_buf, embed,
		  (float*)d_pad_buf, embed,
		  &beta,
		  (float*)d_dist_buf_2, nnz); //1.2 ms
      
      fill_number<<<dim3((voc+1024-1)/1024,batch), 1024>>>((float*)d_dist_buf_1,voc,batch,(float)-1000.0);
      array2dense<<<dim3((nnz+1024-1)/1024,batch), 1024>>>((float*)d_dist_buf_1,(float*)d_dist_buf_2,cscRowIndA,voc,nnz);
      

    }
    
    cudaDeviceSynchronize();
    std::chrono::time_point<std::chrono::system_clock> end2 = std::chrono::system_clock::now();
    
    std::chrono::duration<double> dur = end2 - start2;
    std::cout<<"During:" << dur.count() / Test * 1000 << " ms\n";

    checkCudaError(cudaEventRecord(stop, NULL));
    checkCudaError(cudaEventSynchronize(stop));
    

      
    checkCudaError(cudaEventElapsedTime(&msecTotal, start, stop));
    msecTotal /= Test;
    LOG(INFO)  << "dense2array Time= " << msecTotal << " msec, ";
    checkCudaError(cudaGetLastError());

    int offset = 0;

    std::cout << "d_dist_buf_input\n";
    print_matrix_gpu((float*)d_dist_buf_input, voc, batch, offset, offset+10, 0, batch);

    std::cout << "d_array_buf\n";
    print_matrix_gpu((float*)d_array_buf, voc, 1, offset, offset+10, 0, 1);

    std::cout << "d_index_buf\n";
    print_matrix_gpu((int*)d_index_buf, voc, 1, offset, offset+10, 0, 1);

    std::cout << "nnz\n";
    std::cout<<nnz<<"\n";

    std::cout << "cscRowIndA\n";
    print_matrix_gpu(cscRowIndA,  voc, 1,   offset+0, offset+10, 0, 1 );

    std::cout << "d_db_buf\n";
    print_matrix_gpu((float*)d_db_buf, embed, voc, 992,1001,offset+0, offset+10 );

    std::cout << "d_new_db_buf\n";
    print_matrix_gpu((float *)d_new_db_buf, nnz, embed,offset+0, offset+10,992,1001);

    std::cout << "d_dist_buf_shrink\n";
    print_matrix_gpu((float *)d_dist_buf_2, nnz, batch, 0, 10, offset+0, offset+10 );


    std::cout << "d_dist_buf_output\n";
    print_matrix_gpu((float *)d_dist_buf_1, voc, batch, 0, 10, offset+0, offset+10 );


  }

  
  

  
  /*
    { // self kernel 
    cudaEvent_t start, stop;
    checkCudaError(cudaEventCreate(&start));
    checkCudaError(cudaEventCreate(&stop));
    dim3 threads(block_size, block_size);
    dim3 grid((voc+threads.x-1)/threads.x,
    (batch+threads.y-1)/threads.y);
    int Test = 100;
    checkCudaError(cudaEventRecord(start, NULL));
    for (int i = 0; i < Test; i++) {
    matrixMulCUDA<block_size><<<grid, threads>>>(
    (float*)d_dist_buf_1, (float*)d_dist_buf_input, (float*)d_db_buf, (float*)d_pad_buf,
    voc, batch, embed);
    }
    checkCudaError(cudaEventRecord(stop, NULL));
    checkCudaError(cudaEventSynchronize(stop));
    float msecTotal = 0.0f;
    checkCudaError(cudaEventElapsedTime(&msecTotal, start, stop));
    msecTotal /= Test;
    double flopsPerMatrixMul = 2.0 * voc * batch * embed;
    double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (msecTotal / 1000.0f);
    LOG(INFO) << "Performance= " << gigaFlops << " GFlop/s, "
    << "Time= " << msecTotal << " msec, ";
    checkCudaError(cudaGetLastError());
    }
  */

    { //cublas
    cublasHandle_t cublasHandle;
    checkCublasError(cublasCreate(&cublasHandle));
    float alpha = 1.f, beta = 0.f;
    cudaEvent_t start, stop;
    checkCudaError(cudaEventCreate(&start));
    checkCudaError(cudaEventCreate(&stop));
    dim3 threads(block_size, block_size);
    dim3 grid((voc+threads.x-1)/threads.x,
    (batch+threads.y-1)/threads.y);
    int Test = 100;
    checkCudaError(cudaEventRecord(start, NULL));
    for (int i = 0; i < Test; i++) {
    cublasSgemm(cublasHandle,
    CUBLAS_OP_T, CUBLAS_OP_N,
    voc, batch, embed, &alpha,
    (float*)d_db_buf, embed,
    (float*)d_pad_buf, embed,
    &beta,
    (float*)d_dist_buf_2, voc);
    }
    checkCudaError(cudaEventRecord(stop, NULL));
    checkCudaError(cudaEventSynchronize(stop));
    float msecTotal = 0.0f;
    checkCudaError(cudaEventElapsedTime(&msecTotal, start, stop));
    msecTotal /= Test;
    double flopsPerMatrixMul = 2.0 * voc * batch * embed;
    double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) / (msecTotal / 1000.0f);
    LOG(INFO) << "Performance= " << gigaFlops << " GFlop/s, "
    << "Time= " << msecTotal << " msec, ";
    checkCudaError(cudaGetLastError());

    int offset = 0;
    std::cout << "d_dist_buf_output\n";
    print_matrix_gpu((float *)d_dist_buf_2, voc, batch, 0, 10, offset+0, offset+10 );


    }

    /*
    { //check d_dist_buf_1 == d_dist_buf_2    
    cublasHandle_t cublasHandle;
    checkCublasError(cublasCreate(&cublasHandle));
    float alpha = 1.f, beta = -1.f, result = 1.0f;

    special_matrix_sum<<<1,1024>>>((float*)d_dist_buf_res,(float*) d_dist_buf_1,(float*) d_dist_buf_2,voc*batch);

    cublasSasum(cublasHandle, 1, (float*)d_dist_buf_1, 1, &result);
    checkCudaError(cudaGetLastError());
    std::cout <<"\nFirst element by MatrixMul: "<< result << '\n' ;

    cublasSasum(cublasHandle, 1, (float*)d_dist_buf_2, 1, &result);
    checkCudaError(cudaGetLastError());
    std::cout <<"First element by cublas: " << result << '\n' ;

    cublasSasum(cublasHandle, 1, (float*)d_dist_buf_output, 1, &result);
    checkCudaError(cudaGetLastError());
    std::cout <<"\nFirst element by d_dist_buf_output: "<< result << '\n' ;

    cublasSasum(cublasHandle, voc * batch, (float*)d_dist_buf_res, 1, &result);
    checkCudaError(cudaGetLastError());
    std::cout <<"MatrixMul - cublas (sparse): "<< result << '\n' ;

    std::cout << "d_dist_buf_input\n";
    print_matrix_gpu((float*)d_dist_buf_input, voc, batch, 0, 10, 0, batch);

    std::cout << "d_dist_buf_1\n";
    print_matrix_gpu((float*)d_dist_buf_1, voc, batch, 0, 10, 0, batch);

    std::cout << "d_dist_buf_2\n";
    print_matrix_gpu((float*)d_dist_buf_2, voc, batch, 0, 10, 0, batch);

    //    std::cout << "d_dist_buf_output\n";
    //print_matrix_gpu((float*)d_dist_buf_output, voc, batch, 0, 10, 0, batch);


    }
  */

  return 0;
}
